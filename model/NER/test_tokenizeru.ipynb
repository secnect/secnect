{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30201d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(420)\n",
    "random.seed(420)\n",
    "\n",
    "class LogDataGenerator:\n",
    "    \"\"\"Generate synthetic log data for training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.usernames = ['admin', 'user', 'guest', 'root', 'service', 'test', 'demo', 'john.doe', 'alice', 'bob']\n",
    "        self.ip_ranges = ['192.168.1.', '10.0.0.', '172.16.0.', '203.0.113.', '198.51.100.']\n",
    "        self.ports = ['22', '80', '443', '8080', '3389', '21', '23', '25']\n",
    "        self.durations = ['0.1s', '0.5s', '1.2s', '2.3s', '0.8s', '1.5s', '3.1s']\n",
    "        \n",
    "        self.success_templates = [\n",
    "            \"{timestamp} - Login successful for user {username} from {ip}\",\n",
    "            \"{timestamp} - Authentication succeeded for {username} from {ip} on port {port}\",\n",
    "            \"{timestamp} - User {username} logged in successfully from {ip}\",\n",
    "            \"{timestamp} - Successful login: {username} from {ip} (duration: {duration})\",\n",
    "            \"{timestamp} - Access granted to user {username} from {ip}\",\n",
    "            \"{timestamp} - Logged in successfully for user {username} from {ip}\",\n",
    "            \"{timestamp} - Logged in successfully for user {username} from {ip}\"\n",
    "        ]\n",
    "        \n",
    "        self.failure_templates = [\n",
    "            \"{timestamp} - Login failed for user {username} from {ip}\",\n",
    "            \"{timestamp} - Authentication failed for {username} from {ip}\",\n",
    "            \"{timestamp} - Failed login attempt by {username} from {ip}\",\n",
    "            \"{timestamp} - Access denied for user {username} from {ip} on port {port}\",\n",
    "            \"{timestamp} - Login failure: {username} from {ip} (invalid credentials)\",\n",
    "        ]\n",
    "    \n",
    "    def generate_timestamp(self):\n",
    "        \"\"\"Generate random timestamp\"\"\"\n",
    "        start_date = datetime(2023, 1, 1)\n",
    "        end_date = datetime(2023, 12, 31)\n",
    "        random_date = start_date + timedelta(\n",
    "            seconds=random.randint(0, int((end_date - start_date).total_seconds()))\n",
    "        )\n",
    "        return random_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    def generate_ip(self):\n",
    "        \"\"\"Generate random IP address\"\"\"\n",
    "        return random.choice(self.ip_ranges) + str(random.randint(1, 254))\n",
    "    \n",
    "    def generate_log_entry(self, success: bool) -> str:\n",
    "        \"\"\"Generate a single log entry\"\"\"\n",
    "        template = random.choice(self.success_templates if success else self.failure_templates)\n",
    "        \n",
    "        params = {\n",
    "            'timestamp': self.generate_timestamp(),\n",
    "            'username': random.choice(self.usernames),\n",
    "            'ip': self.generate_ip(),\n",
    "            'port': random.choice(self.ports),\n",
    "            'duration': random.choice(self.durations)\n",
    "        }\n",
    "        \n",
    "        return template.format(**params)\n",
    "    \n",
    "    def generate_dataset(self, n_success: int = 1000, n_failure: int = 1000) -> List[str]:\n",
    "        \"\"\"Generate complete dataset\"\"\"\n",
    "        logs = []\n",
    "        \n",
    "        for _ in range(n_success):\n",
    "            logs.append(self.generate_log_entry(True))\n",
    "    \n",
    "        for _ in range(n_failure):\n",
    "            logs.append(self.generate_log_entry(False))\n",
    "        \n",
    "        random.shuffle(logs)\n",
    "        return logs\n",
    "\n",
    "class BIOTagger:\n",
    "    \"\"\"Convert logs to BIO format for NER training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entity_patterns = {\n",
    "            'TIMESTAMP': r'\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}',\n",
    "            'IP_ADDRESS': r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\n",
    "            'USERNAME': r'(?:user\\s+|for\\s+)([a-zA-Z0-9._-]+)(?:\\s+from|\\s+on|\\s*$)',\n",
    "            'PORT': r'port\\s+(\\d+)',\n",
    "            'DURATION': r'\\(duration:\\s+([0-9.]+s)\\)',\n",
    "            'STATUS': r'(successful|succeeded|successfully|failed|failure|denied|granted)'\n",
    "        }\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization\"\"\"\n",
    "        # Split on whitespace and punctuation, but keep them\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        return tokens\n",
    "    \n",
    "    def tag_entities(self, text: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Tag entities in text using BIO format\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        tags = ['O'] * len(tokens)\n",
    "        \n",
    "        # Join tokens back to text for pattern matching\n",
    "        token_positions = []\n",
    "        current_pos = 0\n",
    "        for token in tokens:\n",
    "            start = text.find(token, current_pos)\n",
    "            end = start + len(token)\n",
    "            token_positions.append((start, end))\n",
    "            current_pos = end\n",
    "        \n",
    "        # Apply entity patterns\n",
    "        for entity_type, pattern in self.entity_patterns.items():\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                start, end = match.span()\n",
    "                \n",
    "                # Find overlapping tokens\n",
    "                entity_tokens = []\n",
    "                for i, (token_start, token_end) in enumerate(token_positions):\n",
    "                    if token_start >= start and token_end <= end:\n",
    "                        entity_tokens.append(i)\n",
    "                \n",
    "                # Apply BIO tagging\n",
    "                if entity_tokens:\n",
    "                    tags[entity_tokens[0]] = f'B-{entity_type}'\n",
    "                    for i in entity_tokens[1:]:\n",
    "                        tags[i] = f'I-{entity_type}'\n",
    "        \n",
    "        return list(zip(tokens, tags))\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for NER data\"\"\"\n",
    "    \n",
    "    def __init__(self, sentences: List[List[Tuple[str, str]]], word2idx: Dict, tag2idx: Dict):\n",
    "        self.sentences = sentences\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        words = [word for word, tag in sentence]\n",
    "        tags = [tag for word, tag in sentence]\n",
    "        \n",
    "        # Convert to indices\n",
    "        word_ids = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in words]\n",
    "        tag_ids = [self.tag2idx[tag] for tag in tags]\n",
    "        \n",
    "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collation function for DataLoader\"\"\"\n",
    "    words, tags = zip(*batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Create lengths\n",
    "    lengths = torch.tensor([len(seq) for seq in words])\n",
    "    \n",
    "    return words_padded, tags_padded, lengths\n",
    "\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    \"\"\"BiLSTM-CRF model for NER\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                           num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Linear layer to map LSTM output to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, sentence, lengths):\n",
    "        # Get embeddings\n",
    "        embeds = self.word_embeds(sentence)\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed_embeds = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.lstm(packed_embeds)\n",
    "        \n",
    "        # Unpack\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Get tag scores\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        return tag_space\n",
    "\n",
    "class NERModel:\n",
    "    \"\"\"Main NER model class\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=100, hidden_dim=128):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = None\n",
    "        self.word2idx = None\n",
    "        self.tag2idx = None\n",
    "        self.idx2tag = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def prepare_data(self, logs: List[str]):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        print(\"Preparing data...\")\n",
    "        \n",
    "        # Generate BIO tags\n",
    "        tagger = BIOTagger()\n",
    "        tagged_sentences = []\n",
    "        \n",
    "        for log in logs:\n",
    "            tagged = tagger.tag_entities(log)\n",
    "            if tagged:  # Only add non-empty sentences\n",
    "                tagged_sentences.append(tagged)\n",
    "        \n",
    "        print(f\"Generated {len(tagged_sentences)} tagged sentences\")\n",
    "        \n",
    "        # Build vocabularies\n",
    "        words = []\n",
    "        tags = []\n",
    "        \n",
    "        for sentence in tagged_sentences:\n",
    "            for word, tag in sentence:\n",
    "                words.append(word.lower())\n",
    "                tags.append(tag)\n",
    "        \n",
    "        # Create word vocabulary\n",
    "        word_counts = Counter(words)\n",
    "        vocab_words = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.most_common()]\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "        \n",
    "        # Create tag vocabulary\n",
    "        unique_tags = list(set(tags))\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "        self.idx2tag = {idx: tag for tag, idx in self.tag2idx.items()}\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        print(f\"Tag set size: {len(self.tag2idx)}\")\n",
    "        print(f\"Tags: {list(self.tag2idx.keys())}\")\n",
    "        \n",
    "        return tagged_sentences\n",
    "    \n",
    "    def create_data_loaders(self, sentences, train_ratio=0.8, batch_size=16):\n",
    "        \"\"\"Create train and validation data loaders\"\"\"\n",
    "        # Split data\n",
    "        split_idx = int(len(sentences) * train_ratio)\n",
    "        train_sentences = sentences[:split_idx]\n",
    "        val_sentences = sentences[split_idx:]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = NERDataset(train_sentences, self.word2idx, self.tag2idx)\n",
    "        val_dataset = NERDataset(val_sentences, self.word2idx, self.tag2idx)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                              shuffle=False, collate_fn=collate_fn)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build the BiLSTM-CRF model\"\"\"\n",
    "        self.model = BiLSTMCRF(\n",
    "            vocab_size=len(self.word2idx),\n",
    "            tagset_size=len(self.tag2idx),\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            hidden_dim=self.hidden_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(f\"Training on {self.device}\")\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch_idx, (words, tags, lengths) in enumerate(train_loader):\n",
    "                words, tags, lengths = words.to(self.device), tags.to(self.device), lengths.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                tag_scores = self.model(words, lengths)\n",
    "                \n",
    "                # Reshape for loss calculation\n",
    "                tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "                tags = tags.view(-1)\n",
    "                \n",
    "                loss = criterion(tag_scores, tags)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for words, tags, lengths in val_loader:\n",
    "                    words, tags, lengths = words.to(self.device), tags.to(self.device), lengths.to(self.device)\n",
    "                    \n",
    "                    tag_scores = self.model(words, lengths)\n",
    "                    tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "                    tags = tags.view(-1)\n",
    "                    \n",
    "                    loss = criterion(tag_scores, tags)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'  Train Loss: {avg_train_loss:.4f}')\n",
    "            print(f'  Val Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "            # Save best model and vocabularies\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                \n",
    "                # Save model state\n",
    "                torch.save({\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'word2idx': self.word2idx,\n",
    "                    'tag2idx': self.tag2idx,\n",
    "                    'idx2tag': self.idx2tag,\n",
    "                    'vocab_size': len(self.word2idx),\n",
    "                    'tagset_size': len(self.tag2idx),\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'hidden_dim': self.hidden_dim\n",
    "                }, 'best_ner_model.pth')\n",
    "                \n",
    "                print(f\"  Model saved with vocab_size={len(self.word2idx)}, tagset_size={len(self.tag2idx)}\")\n",
    "        \n",
    "        # Also save vocabularies separately for easy access\n",
    "        import pickle\n",
    "        with open('vocabularies.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'word2idx': self.word2idx,\n",
    "                'tag2idx': self.tag2idx,\n",
    "                'idx2tag': self.idx2tag\n",
    "            }, f)\n",
    "    \n",
    "    def predict(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Predict entities in a text\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize\n",
    "        tagger = BIOTagger()\n",
    "        tokens = tagger.tokenize(text)\n",
    "        \n",
    "        # Convert to indices\n",
    "        word_ids = [self.word2idx.get(token.lower(), self.word2idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Create tensors\n",
    "        words_tensor = torch.tensor([word_ids]).to(self.device)\n",
    "        lengths_tensor = torch.tensor([len(word_ids)]).to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            tag_scores = self.model(words_tensor, lengths_tensor)\n",
    "            predicted_tags = torch.argmax(tag_scores, dim=2).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Convert back to tags\n",
    "        if len(predicted_tags.shape) == 0:  # Single token\n",
    "            predicted_tags = [predicted_tags.item()]\n",
    "        \n",
    "        predicted_tag_names = [self.idx2tag[tag_id] for tag_id in predicted_tags]\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = {}\n",
    "        current_entity = None\n",
    "        current_tokens = []\n",
    "        \n",
    "        for token, tag in zip(tokens, predicted_tag_names):\n",
    "            if tag.startswith('B-'):\n",
    "                # Save previous entity\n",
    "                if current_entity and current_tokens:\n",
    "                    entities[current_entity] = ' '.join(current_tokens)\n",
    "                \n",
    "                # Start new entity\n",
    "                current_entity = tag[2:]  # Remove 'B-'\n",
    "                current_tokens = [token]\n",
    "            elif tag.startswith('I-') and current_entity == tag[2:]:\n",
    "                current_tokens.append(token)\n",
    "            else:\n",
    "                # Save previous entity\n",
    "                if current_entity and current_tokens:\n",
    "                    entities[current_entity] = ' '.join(current_tokens)\n",
    "                current_entity = None\n",
    "                current_tokens = []\n",
    "        \n",
    "        # Don't forget the last entity\n",
    "        if current_entity and current_tokens:\n",
    "            entities[current_entity] = ' '.join(current_tokens)\n",
    "        \n",
    "        return entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e29f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NER Model for System Log Analysis ===\n",
      "\n",
      "1. Generating synthetic log data...\n",
      "Generated 200 log entries\n",
      "\n",
      "Sample logs:\n",
      "  1. 2023-04-18 02:49:35 - Authentication succeeded for service from 192.168.1.99 on port 23\n",
      "  2. 2023-12-23 07:15:22 - Logged in successfully for user user from 198.51.100.83\n",
      "  3. 2023-08-09 19:32:14 - Logged in successfully for user alice from 192.168.1.69\n",
      "\n",
      "2. Preparing data for training...\n",
      "Preparing data...\n",
      "Generated 200 tagged sentences\n",
      "Vocabulary size: 223\n",
      "Tag set size: 12\n",
      "Tags: ['I-PORT', 'B-STATUS', 'I-TIMESTAMP', 'I-DURATION', 'B-IP_ADDRESS', 'I-USERNAME', 'B-TIMESTAMP', 'O', 'I-IP_ADDRESS', 'B-DURATION', 'B-USERNAME', 'B-PORT']\n",
      "\n",
      "Sample BIO tagged sentence:\n",
      "  2023            B-TIMESTAMP\n",
      "  -               I-TIMESTAMP\n",
      "  04              I-TIMESTAMP\n",
      "  -               I-TIMESTAMP\n",
      "  18              I-TIMESTAMP\n",
      "  02              I-TIMESTAMP\n",
      "  :               I-TIMESTAMP\n",
      "  49              I-TIMESTAMP\n",
      "  :               I-TIMESTAMP\n",
      "  35              I-TIMESTAMP\n",
      "  -               O\n",
      "  Authentication  O\n",
      "  succeeded       B-STATUS\n",
      "  for             B-USERNAME\n",
      "  service         I-USERNAME\n",
      "  from            I-USERNAME\n",
      "  192             B-IP_ADDRESS\n",
      "  .               I-IP_ADDRESS\n",
      "  168             I-IP_ADDRESS\n",
      "  .               I-IP_ADDRESS\n",
      "  1               I-IP_ADDRESS\n",
      "  .               I-IP_ADDRESS\n",
      "  99              I-IP_ADDRESS\n",
      "  on              O\n",
      "  port            B-PORT\n",
      "  23              I-PORT\n",
      "\n",
      "3. Creating data loaders...\n",
      "Train batches: 40\n",
      "Validation batches: 10\n",
      "\n",
      "4. Building and training model...\n",
      "Training on cuda\n",
      "Epoch 1/8:\n",
      "  Train Loss: 1.1015\n",
      "  Val Loss: 0.3927\n",
      "  Model saved with vocab_size=223, tagset_size=12\n",
      "Epoch 2/8:\n",
      "  Train Loss: 0.2118\n",
      "  Val Loss: 0.1166\n",
      "  Model saved with vocab_size=223, tagset_size=12\n",
      "Epoch 3/8:\n",
      "  Train Loss: 0.0700\n",
      "  Val Loss: 0.0407\n",
      "  Model saved with vocab_size=223, tagset_size=12\n",
      "Epoch 4/8:\n",
      "  Train Loss: 0.0269\n",
      "  Val Loss: 0.0162\n",
      "  Model saved with vocab_size=223, tagset_size=12\n",
      "Epoch 5/8:\n",
      "  Train Loss: 0.0138\n",
      "  Val Loss: 0.0099\n",
      "  Model saved with vocab_size=223, tagset_size=12\n",
      "Epoch 6/8:\n",
      "  Train Loss: 0.0082\n",
      "  Val Loss: 0.0062\n",
      "  Model saved with vocab_size=223, tagset_size=12\n",
      "Epoch 7/8:\n",
      "  Train Loss: 0.0055\n",
      "  Val Loss: 0.0044\n",
      "  Model saved with vocab_size=223, tagset_size=12\n",
      "Epoch 8/8:\n",
      "  Train Loss: 0.0043\n",
      "  Val Loss: 0.0033\n",
      "  Model saved with vocab_size=223, tagset_size=12\n",
      "\n",
      "5. Testing inference...\n",
      "\n",
      "Test Results:\n",
      "\n",
      "Test 1:\n",
      "Input: 2023-06-03 14:22:01 - Login succeeded for user guest from 10.0.0.2\n",
      "Extracted entities:\n",
      "  TIMESTAMP: 2023 - 06 - 03 14 : 22 : 01\n",
      "  STATUS: succeeded\n",
      "  USERNAME: user guest from\n",
      "  IP_ADDRESS: 10 . 0 . 0 . 2\n",
      "\n",
      "Test 2:\n",
      "Input: 2023-12-15 09:30:45 - Authentication failed for admin from 192.168.1.100 on port 22\n",
      "Extracted entities:\n",
      "  TIMESTAMP: 2023 - 12 - 15 09 : 30 : 45\n",
      "  STATUS: failed\n",
      "  USERNAME: for admin from\n",
      "  IP_ADDRESS: 192 . 168 . 1 . 100\n",
      "  PORT: 22\n",
      "\n",
      "Test 3:\n",
      "Input: 2023-08-20 16:45:30 - User john.doe logged in successfully from 172.16.0.50\n",
      "Extracted entities:\n",
      "  TIMESTAMP: 2023 - 08 - 20 16 : 45 : 30\n",
      "  IP_ADDRESS: 172 . 16 . 0 . 50\n",
      "\n",
      "=== Training Complete! ===\n",
      "Model saved as 'best_ner_model.pth'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main training and evaluation function\"\"\"\n",
    "    print(\"=== NER Model for System Log Analysis ===\\n\")\n",
    "    \n",
    "    # 1. Generate synthetic data\n",
    "    print(\"1. Generating synthetic log data...\")\n",
    "    generator = LogDataGenerator()\n",
    "    logs = generator.generate_dataset(n_success=100, n_failure=100)\n",
    "    \n",
    "    print(f\"Generated {len(logs)} log entries\")\n",
    "    print(\"\\nSample logs:\")\n",
    "    for i, log in enumerate(logs[:3]):\n",
    "        print(f\"  {i+1}. {log}\")\n",
    "    \n",
    "    # 2. Initialize and prepare data\n",
    "    print(\"\\n2. Preparing data for training...\")\n",
    "    ner_model = NERModel(embedding_dim=100, hidden_dim=256)\n",
    "    tagged_sentences = ner_model.prepare_data(logs)\n",
    "    \n",
    "    # Show sample tagged sentence\n",
    "    print(\"\\nSample BIO tagged sentence:\")\n",
    "    sample_sentence = tagged_sentences[0]\n",
    "    for word, tag in sample_sentence:\n",
    "        print(f\"  {word:15} {tag}\")\n",
    "    \n",
    "    # 3. Create data loaders\n",
    "    print(\"\\n3. Creating data loaders...\")\n",
    "    train_loader, val_loader = ner_model.create_data_loaders(tagged_sentences, batch_size=4)\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # 4. Build and train model\n",
    "    print(\"\\n4. Building and training model...\")\n",
    "    ner_model.build_model()\n",
    "    ner_model.train(train_loader, val_loader, epochs=8, lr=0.001)\n",
    "    \n",
    "    # 5. Test inference\n",
    "    print(\"\\n5. Testing inference...\")\n",
    "    \n",
    "    test_logs = [\n",
    "        \"2023-06-03 14:22:01 - Login succeeded for user guest from 10.0.0.2\",\n",
    "        \"2023-12-15 09:30:45 - Authentication failed for admin from 192.168.1.100 on port 22\",\n",
    "        \"2023-08-20 16:45:30 - User john.doe logged in successfully from 172.16.0.50\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    for i, test_log in enumerate(test_logs):\n",
    "        print(f\"\\nTest {i+1}:\")\n",
    "        print(f\"Input: {test_log}\")\n",
    "        \n",
    "        try:\n",
    "            entities = ner_model.predict(test_log)\n",
    "            print(\"Extracted entities:\")\n",
    "            for entity_type, value in entities.items():\n",
    "                print(f\"  {entity_type}: {value}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "    \n",
    "    print(\"\\n=== Training Complete! ===\")\n",
    "    print(\"Model saved as 'best_ner_model.pth'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabularies:\n",
      "  Vocabulary size: 232\n",
      "  Tag set size: 12\n",
      "  Tags: ['I-PORT', 'B-STATUS', 'I-TIMESTAMP', 'I-DURATION', 'B-IP_ADDRESS', 'I-USERNAME', 'B-TIMESTAMP', 'O', 'I-IP_ADDRESS', 'B-DURATION', 'B-USERNAME', 'B-PORT']\n",
      "Model loaded successfully from best_ner_model.pth\n",
      "Running on: cuda\n",
      "\n",
      "================================================================================\n",
      "TESTING NER MODEL ON LOG ENTRIES\n",
      "================================================================================\n",
      "\n",
      "--- Test Case 1 ---\n",
      "Input Log:\n",
      "  2023-06-03 14:22:01 - Login succeeded for user guest from 10.0.0.2\n",
      "\n",
      "Token-level Predictions:\n",
      "  2023            -> B-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  06              -> I-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  03              -> I-TIMESTAMP\n",
      "  14              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  22              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  01              -> I-TIMESTAMP\n",
      "  -               -> O\n",
      "  Login           -> O\n",
      "  succeeded       -> B-STATUS\n",
      "  for             -> O\n",
      "  user            -> B-USERNAME\n",
      "  guest           -> I-USERNAME\n",
      "  from            -> I-USERNAME\n",
      "  10              -> B-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  0               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  0               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  2               -> I-IP_ADDRESS\n",
      "\n",
      "Extracted Entities:\n",
      "  TIMESTAMP   : 2023 - 06 - 03 14 : 22 : 01\n",
      "  STATUS      : succeeded\n",
      "  USERNAME    : user guest from\n",
      "  IP_ADDRESS  : 10 . 0 . 0 . 2\n",
      "\n",
      "JSON Output:\n",
      "  {\n",
      "  \"TIMESTAMP\": \"2023 - 06 - 03 14 : 22 : 01\",\n",
      "  \"STATUS\": \"succeeded\",\n",
      "  \"USERNAME\": \"user guest from\",\n",
      "  \"IP_ADDRESS\": \"10 . 0 . 0 . 2\"\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Test Case 2 ---\n",
      "Input Log:\n",
      "  2023-12-15 09:30:45 - Authentication failed for admin from 192.168.1.100 on port 22\n",
      "\n",
      "Token-level Predictions:\n",
      "  2023            -> B-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  12              -> I-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  15              -> I-TIMESTAMP\n",
      "  09              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  30              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  45              -> I-TIMESTAMP\n",
      "  -               -> O\n",
      "  Authentication  -> O\n",
      "  failed          -> B-STATUS\n",
      "  for             -> B-USERNAME\n",
      "  admin           -> I-USERNAME\n",
      "  from            -> I-USERNAME\n",
      "  192             -> B-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  168             -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  1               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  100             -> I-IP_ADDRESS\n",
      "  on              -> O\n",
      "  port            -> B-PORT\n",
      "  22              -> B-PORT\n",
      "\n",
      "Extracted Entities:\n",
      "  TIMESTAMP   : 2023 - 12 - 15 09 : 30 : 45\n",
      "  STATUS      : failed\n",
      "  USERNAME    : for admin from\n",
      "  IP_ADDRESS  : 192 . 168 . 1 . 100\n",
      "  PORT        : 22\n",
      "\n",
      "JSON Output:\n",
      "  {\n",
      "  \"TIMESTAMP\": \"2023 - 12 - 15 09 : 30 : 45\",\n",
      "  \"STATUS\": \"failed\",\n",
      "  \"USERNAME\": \"for admin from\",\n",
      "  \"IP_ADDRESS\": \"192 . 168 . 1 . 100\",\n",
      "  \"PORT\": \"22\"\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Test Case 3 ---\n",
      "Input Log:\n",
      "  2023-08-20 16:45:30 - User john.doe logged in successfully from 172.16.0.50\n",
      "\n",
      "Token-level Predictions:\n",
      "  2023            -> B-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  08              -> I-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  20              -> I-TIMESTAMP\n",
      "  16              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  45              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  30              -> I-TIMESTAMP\n",
      "  -               -> O\n",
      "  User            -> O\n",
      "  john            -> O\n",
      "  .               -> O\n",
      "  doe             -> O\n",
      "  logged          -> O\n",
      "  in              -> O\n",
      "  successfully    -> O\n",
      "  from            -> O\n",
      "  172             -> B-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  16              -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  0               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  50              -> I-IP_ADDRESS\n",
      "\n",
      "Extracted Entities:\n",
      "  TIMESTAMP   : 2023 - 08 - 20 16 : 45 : 30\n",
      "  IP_ADDRESS  : 172 . 16 . 0 . 50\n",
      "\n",
      "JSON Output:\n",
      "  {\n",
      "  \"TIMESTAMP\": \"2023 - 08 - 20 16 : 45 : 30\",\n",
      "  \"IP_ADDRESS\": \"172 . 16 . 0 . 50\"\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Test Case 4 ---\n",
      "Input Log:\n",
      "  2023-11-02 11:15:33 - Access denied for user test from 203.0.113.45\n",
      "\n",
      "Token-level Predictions:\n",
      "  2023            -> B-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  11              -> I-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  02              -> I-TIMESTAMP\n",
      "  11              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  15              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  33              -> I-TIMESTAMP\n",
      "  -               -> O\n",
      "  Access          -> O\n",
      "  denied          -> B-STATUS\n",
      "  for             -> O\n",
      "  user            -> B-USERNAME\n",
      "  test            -> I-USERNAME\n",
      "  from            -> I-USERNAME\n",
      "  203             -> B-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  0               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  113             -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  45              -> I-IP_ADDRESS\n",
      "\n",
      "Extracted Entities:\n",
      "  TIMESTAMP   : 2023 - 11 - 02 11 : 15 : 33\n",
      "  STATUS      : denied\n",
      "  USERNAME    : user test from\n",
      "  IP_ADDRESS  : 203 . 0 . 113 . 45\n",
      "\n",
      "JSON Output:\n",
      "  {\n",
      "  \"TIMESTAMP\": \"2023 - 11 - 02 11 : 15 : 33\",\n",
      "  \"STATUS\": \"denied\",\n",
      "  \"USERNAME\": \"user test from\",\n",
      "  \"IP_ADDRESS\": \"203 . 0 . 113 . 45\"\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Test Case 5 ---\n",
      "Input Log:\n",
      "  2023-09-18 08:42:17 - Successful login: alice from 10.0.0.25 (duration: 1.2s)\n",
      "\n",
      "Token-level Predictions:\n",
      "  2023            -> B-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  09              -> I-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  18              -> I-TIMESTAMP\n",
      "  08              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  42              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  17              -> I-TIMESTAMP\n",
      "  -               -> O\n",
      "  Successful      -> B-STATUS\n",
      "  login           -> O\n",
      "  :               -> O\n",
      "  alice           -> O\n",
      "  from            -> O\n",
      "  10              -> B-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  0               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  0               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  25              -> I-IP_ADDRESS\n",
      "  (               -> B-DURATION\n",
      "  duration        -> I-DURATION\n",
      "  :               -> I-DURATION\n",
      "  1               -> I-DURATION\n",
      "  .               -> I-DURATION\n",
      "  2s              -> I-DURATION\n",
      "  )               -> I-DURATION\n",
      "\n",
      "Extracted Entities:\n",
      "  TIMESTAMP   : 2023 - 09 - 18 08 : 42 : 17\n",
      "  STATUS      : Successful\n",
      "  IP_ADDRESS  : 10 . 0 . 0 . 25\n",
      "  DURATION    : ( duration : 1 . 2s )\n",
      "\n",
      "JSON Output:\n",
      "  {\n",
      "  \"TIMESTAMP\": \"2023 - 09 - 18 08 : 42 : 17\",\n",
      "  \"STATUS\": \"Successful\",\n",
      "  \"IP_ADDRESS\": \"10 . 0 . 0 . 25\",\n",
      "  \"DURATION\": \"( duration : 1 . 2s )\"\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Test Case 6 ---\n",
      "Input Log:\n",
      "  2023-07-25 19:28:54 - Failed login attempt by root from 192.168.1.200 on port 3389\n",
      "\n",
      "Token-level Predictions:\n",
      "  2023            -> B-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  07              -> I-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  25              -> I-TIMESTAMP\n",
      "  19              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  28              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  54              -> I-TIMESTAMP\n",
      "  -               -> O\n",
      "  Failed          -> B-STATUS\n",
      "  login           -> O\n",
      "  attempt         -> O\n",
      "  by              -> O\n",
      "  root            -> O\n",
      "  from            -> O\n",
      "  192             -> B-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  168             -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  1               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  200             -> I-IP_ADDRESS\n",
      "  on              -> O\n",
      "  port            -> B-PORT\n",
      "  3389            -> B-PORT\n",
      "\n",
      "Extracted Entities:\n",
      "  TIMESTAMP   : 2023 - 07 - 25 19 : 28 : 54\n",
      "  STATUS      : Failed\n",
      "  IP_ADDRESS  : 192 . 168 . 1 . 200\n",
      "  PORT        : 3389\n",
      "\n",
      "JSON Output:\n",
      "  {\n",
      "  \"TIMESTAMP\": \"2023 - 07 - 25 19 : 28 : 54\",\n",
      "  \"STATUS\": \"Failed\",\n",
      "  \"IP_ADDRESS\": \"192 . 168 . 1 . 200\",\n",
      "  \"PORT\": \"3389\"\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Test Case 7 ---\n",
      "Input Log:\n",
      "  2023-10-12 13:37:22 - Login failure: bob from 172.16.0.99 (invalid credentials)\n",
      "\n",
      "Token-level Predictions:\n",
      "  2023            -> B-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  10              -> I-TIMESTAMP\n",
      "  -               -> I-TIMESTAMP\n",
      "  12              -> I-TIMESTAMP\n",
      "  13              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  37              -> I-TIMESTAMP\n",
      "  :               -> I-TIMESTAMP\n",
      "  22              -> I-TIMESTAMP\n",
      "  -               -> O\n",
      "  Login           -> O\n",
      "  failure         -> B-STATUS\n",
      "  :               -> O\n",
      "  bob             -> O\n",
      "  from            -> O\n",
      "  172             -> B-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  16              -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  0               -> I-IP_ADDRESS\n",
      "  .               -> I-IP_ADDRESS\n",
      "  99              -> I-IP_ADDRESS\n",
      "  (               -> O\n",
      "  invalid         -> O\n",
      "  credentials     -> O\n",
      "  )               -> O\n",
      "\n",
      "Extracted Entities:\n",
      "  TIMESTAMP   : 2023 - 10 - 12 13 : 37 : 22\n",
      "  STATUS      : failure\n",
      "  IP_ADDRESS  : 172 . 16 . 0 . 99\n",
      "\n",
      "JSON Output:\n",
      "  {\n",
      "  \"TIMESTAMP\": \"2023 - 10 - 12 13 : 37 : 22\",\n",
      "  \"STATUS\": \"failure\",\n",
      "  \"IP_ADDRESS\": \"172 . 16 . 0 . 99\"\n",
      "}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "class BIOTagger:\n",
    "    \"\"\"Convert logs to BIO format for NER training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entity_patterns = {\n",
    "            'TIMESTAMP': r'\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}',\n",
    "            'IP_ADDRESS': r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\n",
    "            'USERNAME': r'(?:user\\s+|for\\s+)([a-zA-Z0-9._-]+)(?:\\s+from|\\s+on|\\s*$)',\n",
    "            'PORT': r'port\\s+(\\d+)',\n",
    "            'DURATION': r'\\(duration:\\s+([0-9.]+s)\\)',\n",
    "            'STATUS': r'(successful|succeeded|successfully|failed|failure|denied|granted)'\n",
    "        }\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization\"\"\"\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        return tokens\n",
    "\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    \"\"\"BiLSTM-CRF model for NER\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim=2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Linear layer to map LSTM output to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, sentence, lengths):\n",
    "        # Get embeddings\n",
    "        embeds = self.word_embeds(sentence)\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed_embeds = nn.utils.rnn.pack_padded_sequence(embeds, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(packed_embeds)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        return tag_space\n",
    "\n",
    "class NERModelTester:\n",
    "    \"\"\"Test the trained NER model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='best_ner_model.pth'):\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.word2idx = None\n",
    "        self.tag2idx = None\n",
    "        self.idx2tag = None\n",
    "        self.vocab_size = None\n",
    "        self.tagset_size = None\n",
    "        self.embedding_dim = None\n",
    "        self.hidden_dim = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tagger = BIOTagger()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model and vocabularies\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            print(f\"Error: Model file '{self.model_path}' not found!\")\n",
    "            print(\"Please run the training script first to generate the model.\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Load checkpoint with vocabularies\n",
    "            checkpoint = torch.load(self.model_path, map_location=self.device)\n",
    "            \n",
    "            # Load vocabularies from checkpoint\n",
    "            self.word2idx = checkpoint['word2idx']\n",
    "            self.tag2idx = checkpoint['tag2idx']\n",
    "            self.idx2tag = checkpoint['idx2tag']\n",
    "            self.vocab_size = checkpoint['vocab_size']\n",
    "            self.tagset_size = checkpoint['tagset_size']\n",
    "            self.embedding_dim = checkpoint['embedding_dim']\n",
    "            self.hidden_dim = checkpoint['hidden_dim']\n",
    "            \n",
    "            print(f\"Loaded vocabularies:\")\n",
    "            print(f\"  Vocabulary size: {self.vocab_size}\")\n",
    "            print(f\"  Tag set size: {self.tagset_size}\")\n",
    "            print(f\"  Tags: {list(self.tag2idx.keys())}\")\n",
    "            \n",
    "            # Initialize model with correct dimensions\n",
    "            self.model = BiLSTMCRF(\n",
    "                vocab_size=self.vocab_size,\n",
    "                tagset_size=self.tagset_size,\n",
    "                embedding_dim=self.embedding_dim,\n",
    "                hidden_dim=self.hidden_dim\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Load trained weights\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"Model loaded successfully from {self.model_path}\")\n",
    "            print(f\"Running on: {self.device}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Predict entities in a text\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded!\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tagger.tokenize(text)\n",
    "        \n",
    "        # Convert to indices\n",
    "        word_ids = [self.word2idx.get(token.lower(), self.word2idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Create tensors\n",
    "        words_tensor = torch.tensor([word_ids]).to(self.device)\n",
    "        lengths_tensor = torch.tensor([len(word_ids)]).to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            tag_scores = self.model(words_tensor, lengths_tensor)\n",
    "            predicted_tags = torch.argmax(tag_scores, dim=2).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Convert back to tags\n",
    "        if len(predicted_tags.shape) == 0:  # Single token\n",
    "            predicted_tags = [predicted_tags.item()]\n",
    "        \n",
    "        predicted_tag_names = [self.idx2tag[tag_id] for tag_id in predicted_tags]\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = self._extract_entities(tokens, predicted_tag_names)\n",
    "        \n",
    "        return entities, list(zip(tokens, predicted_tag_names))\n",
    "    \n",
    "    def _extract_entities(self, tokens: List[str], tags: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Extract entities from tokens and tags\"\"\"\n",
    "        entities = {}\n",
    "        current_entity = None\n",
    "        current_tokens = []\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            if tag.startswith('B-'):\n",
    "                # Save previous entity\n",
    "                if current_entity and current_tokens:\n",
    "                    entities[current_entity] = ' '.join(current_tokens)\n",
    "                \n",
    "                # Start new entity\n",
    "                current_entity = tag[2:]  # Remove 'B-'\n",
    "                current_tokens = [token]\n",
    "            elif tag.startswith('I-') and current_entity == tag[2:]:\n",
    "                current_tokens.append(token)\n",
    "            else:\n",
    "                # Save previous entity\n",
    "                if current_entity and current_tokens:\n",
    "                    entities[current_entity] = ' '.join(current_tokens)\n",
    "                current_entity = None\n",
    "                current_tokens = []\n",
    "        \n",
    "        # Don't forget the last entity\n",
    "        if current_entity and current_tokens:\n",
    "            entities[current_entity] = ' '.join(current_tokens)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def test_logs(self, test_logs: List[str]):\n",
    "        \"\"\"Test the model on a list of log entries\"\"\"\n",
    "        if not self.load_model():\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TESTING NER MODEL ON LOG ENTRIES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, log in enumerate(test_logs, 1):\n",
    "            print(f\"\\n--- Test Case {i} ---\")\n",
    "            print(f\"Input Log:\")\n",
    "            print(f\"  {log}\")\n",
    "            \n",
    "            try:\n",
    "                entities, token_tags = self.predict(log)\n",
    "                \n",
    "                print(f\"\\nToken-level Predictions:\")\n",
    "                for token, tag in token_tags:\n",
    "                    print(f\"  {token:15} -> {tag}\")\n",
    "                \n",
    "                print(f\"\\nExtracted Entities:\")\n",
    "                if entities:\n",
    "                    for entity_type, value in entities.items():\n",
    "                        print(f\"  {entity_type:12}: {value}\")\n",
    "                else:\n",
    "                    print(\"  No entities detected\")\n",
    "                \n",
    "                # Format as JSON for easy copying\n",
    "                print(f\"\\nJSON Output:\")\n",
    "                print(f\"  {json.dumps(entities, indent=2)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error during prediction: {e}\")\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    def interactive_test(self):\n",
    "        \"\"\"Interactive testing mode\"\"\"\n",
    "        if not self.load_model():\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INTERACTIVE NER TESTING MODE\")\n",
    "        print(\"Enter log messages to analyze (type 'quit' to exit)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nEnter log message: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                entities, token_tags = self.predict(user_input)\n",
    "                \n",
    "                print(\"\\nToken Analysis:\")\n",
    "                for token, tag in token_tags:\n",
    "                    print(f\"  {token:15} -> {tag}\")\n",
    "                \n",
    "                print(\"\\nExtracted Entities:\")\n",
    "                if entities:\n",
    "                    for entity_type, value in entities.items():\n",
    "                        print(f\"  {entity_type:12}: {value}\")\n",
    "                else:\n",
    "                    print(\"  No entities detected\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main testing function\"\"\"\n",
    "    # Sample test logs\n",
    "    test_logs = [\n",
    "        \"2023-06-03 14:22:01 - Login succeeded for user guest from 10.0.0.2\",\n",
    "        \"2023-12-15 09:30:45 - Authentication failed for admin from 192.168.1.100 on port 22\",\n",
    "        \"2023-08-20 16:45:30 - User john.doe logged in successfully from 172.16.0.50\",\n",
    "        \"2023-11-02 11:15:33 - Access denied for user test from 203.0.113.45\",\n",
    "        \"2023-09-18 08:42:17 - Successful login: alice from 10.0.0.25 (duration: 1.2s)\",\n",
    "        \"2023-07-25 19:28:54 - Failed login attempt by root from 192.168.1.200 on port 3389\",\n",
    "        \"2023-10-12 13:37:22 - Login failure: bob from 172.16.0.99 (invalid credentials)\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = NERModelTester()\n",
    "    \n",
    "    # Test on sample logs\n",
    "    tester.test_logs(test_logs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3445347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tomas\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "succ = []\n",
    "\n",
    "success_words = get_synonyms(\"success\")\n",
    "failure_words = get_synonyms(\"failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a6e9802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bankruptcy', 'failure', 'loser', 'nonstarter', 'unsuccessful_person'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failure_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deca0cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nTraceback (most recent call last):\n  File \"c:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Proces inicializace dynamicky pipojovan knihovny (DLL) se nezdail.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: Proces inicializace dynamicky pipojovan knihovny (DLL) se nezdail.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1999\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:47\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     38\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     46\u001b[0m )\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:68\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_parallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     SUPPORTED_TP_STYLES,\n\u001b[0;32m     66\u001b[0m     shard_and_distribute_module,\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     Conv1D,\n\u001b[0;32m     71\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     prune_linear_layer,\n\u001b[0;32m     77\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Proces inicializace dynamicky pipojovan knihovny (DLL) se nezdail.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-cased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m w1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m w2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:568\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 568\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:388\u001b[0m, in \u001b[0;36m_get_model_class\u001b[1;34m(config, model_mapping)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[1;32m--> 388\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:774\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[0;32m    773\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[0;32m    777\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:788\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:700\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1987\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1985\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1987\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1988\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2001\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2002\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2003\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2004\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nTraceback (most recent call last):\n  File \"c:\\Users\\Tomas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Proces inicializace dynamicky pipojovan knihovny (DLL) se nezdail.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModel.from_pretrained('bert-base-cased', output_hidden_states=True).eval()\n",
    "\n",
    "w1 = \"Failure\"\n",
    "w2 = \"Failed\"\n",
    "w3 = \"Denied\"\n",
    "\n",
    "\n",
    "\n",
    "tok1 = tokenizer(w1, return_tensors=\"pt\")\n",
    "tok2 = tokenizer(w2, return_tensors=\"pt\")\n",
    "tok3 = tokenizer(w3, return_tensors=\"pt\")\n",
    "\n",
    "# Load spaCy model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Words to compare\n",
    "word1 = nlp(\"failure\")\n",
    "word2 = nlp(\"failed\")\n",
    "word3 = nlp(\"denied\")\n",
    "\n",
    "# Compute similarities\n",
    "print(\"failure vs failed:\", word1.similarity(word2))  # ~0.70.9\n",
    "print(\"failure vs denied:\", word1.similarity(word3))  # ~0.40.6\n",
    "\n",
    "print(\"Tok 1 looks like:\", tok1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a13777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
